---
layout: post
title:  Supervise Learning——online learning
date:   2019-08-04 11:02:43 +0800
categories: 课程
tag: 课程笔记
---

* content
{:toc}

# Online Learning（OL）

</br>

## 一、定义OL

### 两个故事情节

- 一个老师在教一个学生学习知识，老师直接教，而是每天让学生回答一个问题，然后老师告诉学生正确答案。学生根据自己的回答和正确答案来更新自己的知识，最终成为了业内大牛。
- 一个IT男和一个邮件编辑，每天编辑都会给IT男发一封垃圾邮件，而苦逼的IT男则是不停更新filter去过滤垃圾邮件，而编辑发的垃圾邮件如果被过滤了，他的下一封邮件会更厉害更难过滤，这样一直较量，IT男的filter就变的越来越强了。

### 什么是OL

上面的两个故事是对OL最贴切的描述了，两个都属于博弈论中的重复游戏，即数据不是一开始全部过来，而是随着时间不断的到来，然后OL算法需要根据新来的数据不停的调整自己的策略。

下面通过对比来分别大致定义Offline Learning和Online Learning：

> **Offline Learning：** 数据的分布是假定为*独立同分布*的，并且直接给出所有数据，从这些数据中学习一个离线的模型去预测新的数据（同分布的），衡量误差为*泛化误差*。
**Online Learning：** 数据的分布是任意的，并且不直接给出所有数据，而是一个*在线的数据序列*，需要使用新输入进来的数据不断的调整模型参数以达到尽可能少犯错的效果，衡量误差为*累积误差*。

通过上面的定义已经能够大概明白什么是OL了，下面给出数学定义：

定义learner和teacher。OL的过程就是teacher不断向learner提问题。在$t$时刻，learner接收到一个问题$x_t$，learner给出一个答案$h_t$，同时teacher告诉learner一个正确答案$y_t$。此时，learner就会计算出一个损失（loss） $l(h_t , y_t)$。learner根据这个损失去更新自己的学习方法，最终learner就学习到了领域知识。过程如下：

1. $t$时刻，teacher提出问题$x_t$。
2. learner从策略集合$\mathcal{H}$中选出一个策略$h_t$，做出判断$h_t(x)$。
3. learner根据$l(h_t(x), y_t)$去进行学习。
4. 如果有更多问题的话，重复第1步。

### regret（后悔值）

由于在线学习不能看到未来的数据，所以在线学习的目标是当学习了所有数据之后，得到的模型策略是最优策略。所以最终学习到的模型策略与最优策略之间的差异称为*后悔*。通俗一点说就是：后悔从一开始就没有选择最优的策略。在线学习中我们不追求最优策略是完美的，我们只追求最终学习到的策略和最优策略没有差异，也就是*不后悔*。

引入后悔的数学定义：

$$R(T) = \sum_{t=1}^T l_t(h_t) - \min_{h\in \mathcal{H}} \sum_{t=1}^T l_t(h)$$

其中$h$为最优策略。learner的目标是每次都挑选合适$h_t$使得$R(t)$最小。在learner不断学习的过程中，只要保证*平均regret*是在不断变小的，我们就可以说这个算法是no-regret的。

> 我们称一个OL算法是No-regret的，意味着：
$$\lim_{T \to \infty} \frac{R(T)}{T} \to 0$$

## 二、OL算法

### 1、专家算法

专家算法是OL算法中最简单的一类算法，实际就是综合**专家**预测的标签和真实标签去进行学习。

给定一组数据序列：
$$S = (x_1, y_1), ..., (x_m, y_m)\in \{0,1\}^n \times \{0,1\}$$

其中每一个$x_t$为一个$n$维向量，代表的是$n$个专家对标签$y_t$的预测值。专家$i$在$t$时刻对标签的预测值为$x_{t,i} \in \{0, 1\}$。然后综合所有专家的预测得出一个标签的预测值$\hat{y}_t$并与真实标签进行$y_t$进行比较得到loss。下图为一个例子：

![图1 专家预测样例](/styles/images/2019-08-04-OL/fig1.png)

在基于专家的算法中，loss的数学定义如下：

$$L_A(S) = \sum_{t=1}^m |y_t - \hat{y}_t|$$

根据这个情景，我们需要去设计专家学习算法，获得小的loss。

#### Halving算法

这个算法考虑了最简单的情况：存在一个完美专家，他永远都能给出正确的答案。那么在这种情况下什么样的策略算是好策略呢？很简单，少数服从多数！听多数专家的话。

learner在$t$时刻选出前$t-1$轮中一直给出正确答案的专家们，然后采用这些专家中多数人给出的答案作为$t$时刻的答案。

我们记$C_t$为前$t-1$轮一直是给正确答案的专家们的集合，$|C_t|$是这些专家的个数。如果learner在时刻$t$给出了错误的答案，那么意味着$C_t$中大部分专家都给了错误答案，那么下一时刻learner参考的专家就会少一半，即$|C_{t+1}|\le \frac{|C_t|}{2}$。因为完美专家的存在，所以$|C_t|$不可能无限变小使得小于1，所以learner不会无限犯错。实际上专家个数至多有$\log_2(n)$次变小机会，所以learner最多有$\log_2(n)$的损失。另一方面，最优策略当然是一直选中一位完美专家，总损失为0，所以regret的上界是个常数：
$$R(T)\le \log_2(n)$$

#### Weighted majority 算法

如果没有完美专家了，该怎么办呢？

我们对每个专家$E_i$都维护一个置信度$w_i \in \{0,1\}$，满足$\sum_{i=1}^n w_i = 1$。在$t$时刻，所有的专家进行投票，我们根据每个专家的置信度$w_{t, i}$，选出总置信度大于0.5的那些专家预测的标签作为$t$时刻的预测标签。

这里就涉及到一个问题，如何更新$w_i$？

直观来看，每轮预测后，我们要增大预测对的专家的置信度，降低预测错的专家的置信度。所以我们可以通过乘比例因子的方法来更新置信度。

定义指数$\beta \in [0, 1\}$，每轮预测错误的专家，我们都将他的置信度乘上$\beta$，